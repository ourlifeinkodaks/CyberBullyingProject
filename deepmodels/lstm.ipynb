{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf9d966-a1fa-4c39-9a5a-259f66f31a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Libraries for general purpose\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Data preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "#PyTorch LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#Tokenization for LSTM\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "#Seed for reproducibility\n",
    "import random\n",
    "\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "import time\n",
    "\n",
    "#set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6107f1de-8476-40fc-ac7d-090ee21645b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../model_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6223da8-18e3-4f75-b48a-021ddd21d551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>lemmatized_tweets</th>\n",
       "      <th>polarity_nltk</th>\n",
       "      <th>polarity_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>words katandandre food crapilicious</td>\n",
       "      <td>word katandandre food crapilicious</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>aussietv white theblock imacelebrityau today s...</td>\n",
       "      <td>aussietv white theblock imacelebrityau today s...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>classy whore red velvet cupcakes</td>\n",
       "      <td>classy whore red velvet cupcake</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>meh p thanks heads concerned another angry dud...</td>\n",
       "      <td>meh p thanks head concerned another angry dude...</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>-0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "      <td>isis account pretending kurdish account like i...</td>\n",
       "      <td>isi account pretending kurdish account like is...</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type  \\\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying   \n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying   \n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying   \n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying   \n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying   \n",
       "\n",
       "                                        clean_tweets  \\\n",
       "0                words katandandre food crapilicious   \n",
       "1  aussietv white theblock imacelebrityau today s...   \n",
       "2                   classy whore red velvet cupcakes   \n",
       "3  meh p thanks heads concerned another angry dud...   \n",
       "4  isis account pretending kurdish account like i...   \n",
       "\n",
       "                                   lemmatized_tweets  polarity_nltk  \\\n",
       "0                 word katandandre food crapilicious         0.0000   \n",
       "1  aussietv white theblock imacelebrityau today s...         0.0000   \n",
       "2                    classy whore red velvet cupcake        -0.3400   \n",
       "3  meh p thanks head concerned another angry dude...        -0.1779   \n",
       "4  isi account pretending kurdish account like is...         0.4404   \n",
       "\n",
       "   polarity_textblob  \n",
       "0               0.00  \n",
       "1               0.00  \n",
       "2               0.05  \n",
       "3              -0.15  \n",
       "4               0.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97e7f82-3904-4320-8df5-fda8fdfc2db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41310588-5046-432b-bf71-1fc85a497955",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_new = []\n",
    "for t in df.clean_tweets:\n",
    "    texts_new.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060a0c43-f649-4a90-8638-b785ccba6695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2440"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"clean_tweets\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e4029a-38a5-4441-b2cf-e85e0a710f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(\"clean_tweets\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfe8983-790f-4e07-b924-13a79dfa403f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44901, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee9971d-2cbf-4aff-8406-5954d0652a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "religion               7958\n",
       "age                    7891\n",
       "ethnicity              7810\n",
       "not_cyberbullying      7711\n",
       "gender                 7648\n",
       "other_cyberbullying    5883\n",
       "Name: cyberbullying_type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cyberbullying_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1469f00-8d73-4553-802c-da2bd495b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(column, seq_len):\n",
    "    ##Create vocabulary of words from column\n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    count_words = Counter(corpus)\n",
    "    sorted_words = count_words.most_common()\n",
    "    vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "    ##Tokenize the columns text using the vocabulary\n",
    "    text_int = []\n",
    "    for text in column:\n",
    "        r = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(r)\n",
    "    ##Add padding to tokens\n",
    "    features = np.zeros((len(text_int), seq_len), dtype = int)\n",
    "    for i, review in enumerate(text_int):\n",
    "        if len(review) <= seq_len:\n",
    "            zeros = list(np.zeros(seq_len - len(review)))\n",
    "            new = zeros + review\n",
    "        else:\n",
    "            new = review[: seq_len]\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return sorted_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6dd7b4e9-076d-46b4-97c4-9da2fef97c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, tokenized_column = Tokenize(df[\"clean_tweets\"], 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "932495e0-63bd-4f08-be6f-17046dd2f765",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_codes = {'age':1, \n",
    "               'gender':2, \n",
    "               'ethnicity':3, \n",
    "               'religion':4, \n",
    "               'other_cyberbullying':5, \n",
    "               'not_cyberbullying':0\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a589aa15-21c6-4b86-be7d-433c2202644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding target labels\n",
    "df['cyberbullying_type'] = df['cyberbullying_type'].replace(label_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a53bde3-480a-42c9-830f-4230ab363f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "for key, value in vocabulary[:20]:\n",
    "    keys.append(key)\n",
    "    values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adeb9a31-46c9-49b9-888c-cd60f333b593",
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e88bfef8-f4d4-4f5c-98e3-9940c40a53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10b080e6-a972-4f9a-9888-4ea12efc45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary) + 1 #+1 for the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7821e75c-31f2-40de-b7b4-95277911698e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Matrix Shape: (49716, 200)\n"
     ]
    }
   ],
   "source": [
    "#define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "#fill the embedding matrix with the pre trained values from word2vec\n",
    "#    corresponding to word (string), token (number associated to the word)\n",
    "for word, token in vocabulary:\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "print(\"Embedding Matrix Shape:\", embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4eded4a7-9aec-4e47-a00d-710087154645",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenized_column\n",
    "y = df['cyberbullying_type'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69b88683-53b0-4988-b070-6043e968d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24a385e7-47a7-4b55-9c2c-5cb5e029808d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, \n",
    "                                                      test_size=0.1, \n",
    "                                                      stratify=y_train, \n",
    "                                                      random_state=seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "860f1d49-e98f-48d8-8e85-23e041b6dbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling the training set\n",
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(np.array(X_train),np.array(y_train));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6545d7f3-f7df-4bfc-8ef2-84c3b48aa217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pytorch datasets\n",
    "train_data = TensorDataset(torch.from_numpy(X_train_os), torch.from_numpy(y_train_os))\n",
    "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "valid_data = TensorDataset(torch.from_numpy(X_valid), torch.from_numpy(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "60a68c36-5b26-486c-8a5d-308c135d9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True) \n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4c053f7-b93e-4d8f-8b69-c9d2c73ef1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6 #We are dealing with a multiclass classification of 5 classes\n",
    "HIDDEN_DIM = 100 #number of neurons of the internal state (internal neural network in the LSTM)\n",
    "LSTM_LAYERS = 1 #Number of stacked LSTM layers\n",
    "\n",
    "LR = 3e-4 #Learning rate\n",
    "DROPOUT = 0.5 #LSTM Dropout\n",
    "BIDIRECTIONAL = True #Boolean value to choose if to use a bidirectional LSTM or not\n",
    "EPOCHS = 5 #Number of training epoch\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7536899-b904-4658-9931-7f79649581d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_Sentiment_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, lstm_layers, bidirectional,batch_size, dropout):\n",
    "        super(BiLSTM_Sentiment_Classifier,self).__init__()\n",
    "        \n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        self.batch_size = x.size(0)\n",
    "        ##EMBEDDING LAYER\n",
    "        embedded = self.embedding(x)\n",
    "        #LSTM LAYERS\n",
    "        out, hidden = self.lstm(embedded, hidden)\n",
    "        #Extract only the hidden state from the last LSTM cell\n",
    "        out = out[:,-1,:]\n",
    "        #FULLY CONNECTED LAYERS\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        #Initialization of the LSTM hidden and cell states\n",
    "        h0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        c0 = torch.zeros((self.lstm_layers*self.num_directions, batch_size, self.hidden_dim)).detach().to(DEVICE)\n",
    "        hidden = (h0, c0)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ded90ae6-f63a-4936-b3e8-294eee709f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Sentiment_Classifier(\n",
      "  (embedding): Embedding(49716, 200)\n",
      "  (lstm): LSTM(200, 100, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=200, out_features=6, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_Sentiment_Classifier(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM,NUM_CLASSES, LSTM_LAYERS,BIDIRECTIONAL, BATCH_SIZE, DROPOUT)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "#Initialize embedding with the previously defined embedding matrix\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "#Allow the embedding matrix to be fined tuned to better adapt to out dataset and get higher accuracy\n",
    "model.embedding.weight.requires_grad=True\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5597864e-bc1b-4dca-9e1c-50093844e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay = 5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b904555-a9c0-4526-b079-a1148b54cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:Validation accuracy increased (0.000000 --> 77.176339).  Saving model ...\n",
      "\tTrain_loss : 0.9502 Val_loss : 0.4905\n",
      "\tTrain_acc : 60.443% Val_acc : 77.176%\n",
      "Epoch 2:Validation accuracy increased (77.176339 --> 83.677455).  Saving model ...\n",
      "\tTrain_loss : 0.4036 Val_loss : 0.4100\n",
      "\tTrain_acc : 83.485% Val_acc : 83.677%\n",
      "Epoch 3:Validation accuracy increased (83.677455 --> 84.123884).  Saving model ...\n",
      "\tTrain_loss : 0.2444 Val_loss : 0.4154\n",
      "\tTrain_acc : 91.067% Val_acc : 84.124%\n",
      "Epoch 4:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.1444 Val_loss : 0.4705\n",
      "\tTrain_acc : 94.972% Val_acc : 83.650%\n",
      "Epoch 5:Validation accuracy did not increase\n",
      "\tTrain_loss : 0.0928 Val_loss : 0.5535\n",
      "\tTrain_acc : 96.936% Val_acc : 82.589%\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "\n",
    "early_stopping_patience = 4\n",
    "early_stopping_counter = 0\n",
    "\n",
    "valid_acc_max = 0 # Initialize best accuracy top 0\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "\n",
    "    #lists to host the train and validation losses of every batch for each epoch\n",
    "    train_loss, valid_loss  = [], []\n",
    "    #lists to host the train and validation accuracy of every batch for each epoch\n",
    "    train_acc, valid_acc  = [], []\n",
    "\n",
    "    #lists to host the train and validation predictions of every batch for each epoch\n",
    "    y_train_list, y_val_list = [], []\n",
    "\n",
    "    #initalize number of total and correctly classified texts during training and validation\n",
    "    correct, correct_val = 0, 0\n",
    "    total, total_val = 0, 0\n",
    "    running_loss, running_loss_val = 0, 0\n",
    "\n",
    "\n",
    "    ####TRAINING LOOP####\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE) #load features and targets in device\n",
    "\n",
    "        h = model.init_hidden(labels.size(0))\n",
    "\n",
    "        model.zero_grad() #reset gradients \n",
    "\n",
    "        output, h = model(inputs,h) #get output and hidden states from LSTM network\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_train = torch.argmax(output, dim=1) #get tensor of predicted values on the training set\n",
    "        y_train_list.extend(y_pred_train.squeeze().tolist()) #transform tensor to list and the values to the list\n",
    "        \n",
    "        correct += torch.sum(y_pred_train==labels).item() #count correctly classified texts per batch\n",
    "        total += labels.size(0) #count total texts per batch\n",
    "\n",
    "    train_loss.append(running_loss / total_step)\n",
    "    train_acc.append(100 * correct / total)\n",
    "\n",
    "    ####VALIDATION LOOP####\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            val_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "            output, val_h = model(inputs, val_h)\n",
    "\n",
    "            val_loss = criterion(output, labels)\n",
    "            running_loss_val += val_loss.item()\n",
    "\n",
    "            y_pred_val = torch.argmax(output, dim=1)\n",
    "            y_val_list.extend(y_pred_val.squeeze().tolist())\n",
    "\n",
    "            correct_val += torch.sum(y_pred_val==labels).item()\n",
    "            total_val += labels.size(0)\n",
    "\n",
    "        valid_loss.append(running_loss_val / total_step_val)\n",
    "        valid_acc.append(100 * correct_val / total_val)\n",
    "\n",
    "    #Save model if validation accuracy increases\n",
    "    if np.mean(valid_acc) >= valid_acc_max:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print(f'Epoch {e+1}:Validation accuracy increased ({valid_acc_max:.6f} --> {np.mean(valid_acc):.6f}).  Saving model ...')\n",
    "        valid_acc_max = np.mean(valid_acc)\n",
    "        early_stopping_counter=0 #reset counter if validation accuracy increases\n",
    "    else:\n",
    "        print(f'Epoch {e+1}:Validation accuracy did not increase')\n",
    "        early_stopping_counter+=1 #increase counter if validation accuracy does not increase\n",
    "        \n",
    "    if early_stopping_counter > early_stopping_patience:\n",
    "        print('Early stopped at epoch :', e+1)\n",
    "        break\n",
    "    \n",
    "    print(f'\\tTrain_loss : {np.mean(train_loss):.4f} Val_loss : {np.mean(valid_loss):.4f}')\n",
    "    print(f'\\tTrain_acc : {np.mean(train_acc):.3f}% Val_acc : {np.mean(valid_acc):.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a802567-9ecd-496c-ba23-7a604b2386dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1bd7a4f7-bdc5-4427-9d88-719c3f88e52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "y_pred_list = []\n",
    "y_test_list = []\n",
    "for inputs, labels in test_loader:\n",
    "    inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "    test_h = model.init_hidden(labels.size(0))\n",
    "\n",
    "    output, val_h = model(inputs, test_h)\n",
    "    y_pred_test = torch.argmax(output, dim=1)\n",
    "    y_pred_list.extend(y_pred_test.squeeze().tolist())\n",
    "    y_test_list.extend(labels.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "030723a1-bce7-4f37-a3ba-16aba009fedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report for multiclass classification\n",
    "report = classification_report(y_test_list, y_pred_list, output_dict=True)\n",
    "class_report = pd.DataFrame(report).transpose()\n",
    "class_report['cuberbullying_type'] = ['not_cyberbulling', 'age', 'gender',\n",
    "                                     'ethnicity', 'religion', 'other_cyberbullying', '', '', '']\n",
    "class_report = class_report[['cuberbullying_type', 'precision', 'recall',\n",
    "                            'f1-score', 'support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94a4e31c-00ea-44dd-a646-fbed6589a5d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cuberbullying_type</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not_cyberbulling</td>\n",
       "      <td>0.627515</td>\n",
       "      <td>0.629148</td>\n",
       "      <td>0.628330</td>\n",
       "      <td>1537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>0.963125</td>\n",
       "      <td>0.979657</td>\n",
       "      <td>0.971321</td>\n",
       "      <td>1573.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gender</td>\n",
       "      <td>0.830374</td>\n",
       "      <td>0.889180</td>\n",
       "      <td>0.858771</td>\n",
       "      <td>1525.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ethnicity</td>\n",
       "      <td>0.978247</td>\n",
       "      <td>0.980128</td>\n",
       "      <td>0.979187</td>\n",
       "      <td>1560.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>religion</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.956576</td>\n",
       "      <td>0.948814</td>\n",
       "      <td>1589.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_cyberbullying</td>\n",
       "      <td>0.601190</td>\n",
       "      <td>0.515306</td>\n",
       "      <td>0.554945</td>\n",
       "      <td>1176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td></td>\n",
       "      <td>0.839174</td>\n",
       "      <td>0.839174</td>\n",
       "      <td>0.839174</td>\n",
       "      <td>0.839174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td></td>\n",
       "      <td>0.823605</td>\n",
       "      <td>0.824999</td>\n",
       "      <td>0.823561</td>\n",
       "      <td>8960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td></td>\n",
       "      <td>0.834196</td>\n",
       "      <td>0.839174</td>\n",
       "      <td>0.836057</td>\n",
       "      <td>8960.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cuberbullying_type  precision    recall  f1-score      support\n",
       "0                not_cyberbulling   0.627515  0.629148  0.628330  1537.000000\n",
       "1                             age   0.963125  0.979657  0.971321  1573.000000\n",
       "2                          gender   0.830374  0.889180  0.858771  1525.000000\n",
       "3                       ethnicity   0.978247  0.980128  0.979187  1560.000000\n",
       "4                        religion   0.941176  0.956576  0.948814  1589.000000\n",
       "5             other_cyberbullying   0.601190  0.515306  0.554945  1176.000000\n",
       "accuracy                            0.839174  0.839174  0.839174     0.839174\n",
       "macro avg                           0.823605  0.824999  0.823561  8960.000000\n",
       "weighted avg                        0.834196  0.839174  0.836057  8960.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_report"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
